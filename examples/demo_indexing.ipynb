{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a23f57a",
   "metadata": {},
   "source": [
    "Load content and attached metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c6c08de",
   "metadata": {},
   "outputs": [],
   "source": [
    "content1 = \"This service package manages a fleet of commercial vehicles. The Fleet and Freight Management Center monitors the vehicle fleet and can provide routes using either an in-house capability or an external provider. Routes generated by either approach are constrained by hazardous materials and other restrictions (such as height or weight). A route is electronically sent to the Commercial Vehicle with any appropriate dispatch instructions. The location of the Commercial Vehicle can be monitored by the Fleet and Freight Management Center and routing changes can be made depending on current road network conditions. This service package also supports maintenance of fleet vehicles with on-board monitoring equipment. Records of vehicle mileage, preventative maintenance and repairs are maintained\"\n",
    "metadata1 = {'item type' : 'service', 'name' : 'carrier operations and fleet management'}\n",
    "\n",
    "content2 = \"The 'Basic Commercial Vehicle' represents the commercial vehicle that hosts the on-board equipment that provides ITS capabilities. It includes the heavy vehicle databus and all other interface points between on-board systems and the rest of the commercial vehicle. This vehicle is used to transport goods, is operated by a professional driver and typically administered as part of a larger fleet. Commercial Vehicle classification applies to all goods transport vehicles ranging from small panel vans used in local pick-up and delivery services to large, multi-axle tractor-trailer rigs operating on long haul routes.\"\n",
    "metadata2 = {'item type' : 'physical', 'name' : 'basic commercial vehicle', 'kind' : 'terminator', 'class' : 'vehicle', 'service' : ['carrier operations and fleet management', 'freight administration']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76cccf0",
   "metadata": {},
   "source": [
    "Build Documents (for index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba6417b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "doc1 = Document(page_content=content1, metadata=metadata1)\n",
    "\n",
    "doc2 = Document(page_content=content2, metadata=metadata2)\n",
    "\n",
    "docs = [doc1, doc2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62ad2b1",
   "metadata": {},
   "source": [
    "Assign an id to each document (link between collection and vector index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acee3f1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "def assign_ids(docs:list[Document]):\n",
    "    for doc in docs:\n",
    "        doc.metadata[\"id\"] = str(uuid.uuid4())\n",
    "    return(docs)\n",
    "\n",
    "docs_with_id = assign_ids(docs)\n",
    "len(docs_with_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da10f374",
   "metadata": {},
   "source": [
    "Chunk documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4054973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 512,\n",
    "    chunk_overlap = 100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "docs_chunked = splitter.split_documents(docs_with_id)\n",
    "len(docs_chunked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8854fe",
   "metadata": {},
   "source": [
    "Instantiate embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8023901",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mateo.becquart\\Documents\\Travail\\explo-rag-sti\\bleurt-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\mateo.becquart\\Documents\\Travail\\explo-rag-sti\\bleurt-env\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Alibaba-NLP/gte-multilingual-base were not used when initializing NewModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model =  HuggingFaceEmbeddings( # Instantiate the embedding method\n",
    "        model_name=\"Alibaba-NLP/gte-multilingual-base\",     \n",
    "        model_kwargs={\"device\" : 'cpu', \"trust_remote_code\" : True},\n",
    "        encode_kwargs={'normalize_embeddings': True} \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cae423",
   "metadata": {},
   "source": [
    "Instantiate vector index (similarity search) and collection (complex pre-filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e252f861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector db :  0\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from pymongo import MongoClient\n",
    "\n",
    "index_path = \"../data/demo\" # We use a persistent directory for our chroma db. This way, data is saved between calls\n",
    "vector_db = Chroma(\n",
    "    collection_name=\"demo_db\",\n",
    "    embedding_function=embedding_model,\n",
    "    persist_directory=index_path,\n",
    ")\n",
    "\n",
    "print(\"vector db : \", vector_db._collection.count())\n",
    "\n",
    "mongo_path = \"mongodb://192.168.211.96:27017/\" # \n",
    "client = MongoClient(mongo_path)\n",
    "metadata_db = client[\"metadata_db\"][\"demo_collection\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06be5af",
   "metadata": {},
   "source": [
    "Store in the index the content and the id, store in the collection the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a56e07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_chroma(chroma_db, docs:list[Document]):\n",
    "    \"\"\" Store only content and id in the vector db \"\"\"\n",
    "    chroma_docs = [] # Without any list in metadata\n",
    "    for doc in docs:\n",
    "        new_doc = Document(\n",
    "            page_content=doc.page_content,\n",
    "            metadata={'id' : doc.metadata['id'], 'name' : doc.metadata['name']}  \n",
    "        )\n",
    "        chroma_docs.append(new_doc)\n",
    "\n",
    "    chroma_db.add_documents(chroma_docs)\n",
    "\n",
    "def batchify(docs:list[Document], batch_size): # Max batch size is 5400 for chroma db\n",
    "    \"\"\" Chroma db has a max batch size of about 5k, index batches and not the full corpus at once \"\"\"\n",
    "    for i in range(0, len(docs), batch_size):\n",
    "        yield docs[i:i + batch_size]\n",
    "\n",
    "def doc_into_dict(docs:list[Document])->list:\n",
    "    \"\"\" Mongo collection stores dictionnaries and not Document objects \"\"\"\n",
    "    return([\n",
    "        {'content' : doc.page_content,\n",
    "         'metadata' : doc.metadata}\n",
    "         for doc in docs\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23ee9bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection_docs = doc_into_dict(docs_chunked) # Mongo collection stores dict and not Document\n",
    "len(collection_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d063c1f",
   "metadata": {},
   "source": [
    "Embedding and indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e618605d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "metadata_db.insert_many(collection_docs)\n",
    "\n",
    "for batch in batchify(docs_chunked, 5000):\n",
    "    index_chroma(vector_db, batch)\n",
    "print(vector_db._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9b3f15",
   "metadata": {},
   "source": [
    "The index contains the documents' embeddings. \n",
    "The collection contains the documents' metadata.\n",
    "We will perform similarity search on the former and pre-filtering on the latter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bleurt-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
